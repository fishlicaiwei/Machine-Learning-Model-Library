import random
import torch

# 数据处理好了，接下来来定义模型
w=torch.normal(0,0.01,size=(2,1),requires_grad=True)
b=torch.zeros(1,requires_grad=True)
# 初始化模型

def linreg(x,w,b):
    """线性回归模型"""
    return torch.matmul(x,w)+b

# import torch
# # 输入张量 x (2个样本，每个样本3个特征)
# x = torch.tensor([[1.0, 2.0, 3.0],
#                  [4.0, 5.0, 6.0]])
# # 权重矩阵 w (3个输入特征，2个输出特征)
# w = torch.tensor([[0.1, 0.2],
#                   [0.3, 0.4],
#                   [0.5, 0.6]])
# # 偏置向量 b (2个输出特征)
# b = torch.tensor([0.1, 0.2])
# # 计算
# result = torch.matmul(x, w) + b
# print(result)
# [[2.3, 3.0],
#  [5.0, 6.6]]

# 这个linreg函数最后的结果是矩阵。他是 x张量，样本特征 经过线性变换后的结果。而输出结果可以直接作为一层神经网络，而作为下一行的输入。比如这个例子。输入的x是三个特征
# 经过线性变化之后出入的结果是两个特征。下一次输入就是两个特征


def squared_loss(y_hat,y):
    """均方损失。"""
    return (y_hat-y.reshape(y_hat.shape))**2/2

# 这个均方损失函数“比较原始”，没有计算总和，也没有计算平均。最后得到的是 （差值）平方/2,**2是2次幂。得到的结果是矩阵


# 定义优化算法，成本函数下降的迭代算法
def sgd(params,lr,barch_size):
     with torch.no_grad():
         for param in params:
             param -=lr*param.grad/batch_size
         # 梯度要进行归一化
             param.grad.zero_()
          # 把梯度清零，下次计算的时候就不会和上一次相关

# 初始化数据
# 训练模型
lr=0.03
num_epochs=3
net=linreg
loss=squared_loss
# 对学习率进行初始化，把整个数据扫三遍，初始模型，如现在是线性回归 limreg，损失函数调用均方损失


# 梯度下降的实现。如，num_epochs=3,对整个数据集（即所有小批次）进行3次循环。整个过程中的迭代次数是num_epochs* num_examples/batch_size次，不是3
for epoch in range(num_epochs):
    for x,y in data_iter(batch_size,features,labels):
        # 每次循环得到的就是一个批次，而data_iter每次返回的是两个张量（矩阵？）一个是batch_size个example的features（x1，x2，x3，，，xi等）
        # feature的张量可能是多维的（如，矩阵形式）。另一个是labels张量如（y1，y2，y3）
        # x，y取的是整个当前批量的特征张量，y是当前批量的标签张量
        l=loss(net(x,w,b),y)
        # 先对整个张量（批量）里的所有数据带入模型进行训练，再把训练过的结果计算损失值，这个时候的损失值也是一个张量，
        # 因为损失函数的定义如此：def linreg(x,w,b):   return torch.matmul(x,w)+b
   
        
        l.sum().backward()
        # 计算梯度。注意之前loss函数的设定没有计算总和.向前传播是计算预测y值，向后传播是计算梯度，计算l.sum对所有参数的梯度。没有求和之前l是一个张量，不好计算梯度。
        sgd([w,b],lr,batch_size)
        # 更新参数
     # for循环要遍历所有的批次，每个批次都要重新计算一次损失值，计算一次梯度，更新一次参数。下一个批次更新时，使用的是上一个批次的参数。只有完整的进行完一次过程。才是一次
     # 迭代。目的是要更新参数，参与到下一次迭代之中。而这次的梯度也就可以释放了。

    
# 小批量随机梯度下降（Mini-batch Stochastic Grandient Descent,SGD）
# 梯度下降的本质是进行迭代。迭代可以运用同一组数据。而小批量随机梯度下降的思想是运用不同批次的数据进行迭代。如果只有一个样本，而不是一个批次的样本，y-yhat即公式，不用求和
# 能够更快找到合适的方向，减少单个样本偏差带来的震荡。但对比y-yhat的总和太多的大批次来说能减少内存消耗。

    with torch.no_grad():
        # torch.no_grad()，在机器学习模型中，一个方面是基于之前计算好的梯度。进行如，更新参数（用刚刚计算的梯度），以及在测试集中向前传播，计算损失（不需要梯度的）
        #每次反向传播后，参数的梯度会在默认情况下累积。每次调用.backward（）时，梯度会加到之前的梯度上。因此在参数更新后需要将梯度清零避免累计。
             train_l=loss(net(features,w,b),labels)
             #features和labels调用的是整个数据集。计算最后的w，b的预测值与真实值之间的损失
             # 注意这个loss函数是没有求和及平均的，是一个张量，之后需要求.sum  .mean
             print(f'epoch{epoch+1},loss{float(train_l.mean()):f}')
        
